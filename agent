import json
import operator
from typing import Annotated, Sequence, TypedDict, Union, List
from pathlib import Path
import tempfile  # <--- æ–°å¢žå¼•å…¥
import shutil
# LangGraph ç›¸å…³å¼•å…¥
from langgraph.graph import StateGraph, START, END
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage

# LangChain ç›¸å…³å¼•å…¥
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.prompts import ChatPromptTemplate

# å¼•å…¥ä¹‹å‰çš„ RAG ç³»ç»Ÿ
from rag_system import MultiDocumentKnowledgeBase, UnstructuredPDFParser

# ================= é…ç½®åŒºåŸŸ =================
FINE_TUNED_MODEL_NAME = "gpt-5-1"  # æ›¿æ¢æ‚¨çš„æ¨¡åž‹åç§°
MODEL_API_BASE = "https://api.zchat.tech/v1"  # æ‚¨çš„æ¨¡åž‹ API åœ°å€
MODEL_API_KEY = "sk-WxdUVsxe1Nlsg8w9ViV1NFiszAnloxXuSk7yMl6cVCujrQKj"  # æ‚¨çš„ API Key
KB_STORAGE_PATH = "./multimodel_doc_kb_memory_qwen_chunk"
# ===========================================

# 1. åˆå§‹åŒ– LLM å’Œ çŸ¥è¯†åº“
print("æ­£åœ¨åˆå§‹åŒ– LLM å’ŒçŸ¥è¯†åº“...")
llm = ChatOpenAI(
    model=FINE_TUNED_MODEL_NAME,
    openai_api_key=MODEL_API_KEY,
    openai_api_base=MODEL_API_BASE,
    temperature=0.1,
    streaming=True
)

kb = MultiDocumentKnowledgeBase(
    kb_path=KB_STORAGE_PATH,
    openai_api_key=MODEL_API_KEY,
    llm_instance=llm
)
print("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆã€‚")


# ================= å®šä¹‰å·¥å…· (Tools) =================
# è¿™äº›å·¥å…·ä¸Žä¹‹å‰é€»è¾‘ä¸€è‡´ï¼Œåªæ˜¯è¢« LangGraph è°ƒç”¨

@tool
def course_qa_tool(query: str):
    """ã€è¯¾ç¨‹é—®ç­”å·¥å…·ã€‘ç”¨äºŽå›žç­”é€å¹³æœºæ¢°åŽŸç†çš„ä¸“ä¸šé—®é¢˜ã€‚è¾“å…¥å®Œæ•´é—®é¢˜ã€‚"""
    result = kb.query(query, k=5, rerank_top_n=3)
    return result['answer']


@tool
def generate_quiz_tool(topic: str, difficulty: str = "ä¸­ç­‰", question_count: int = 3):
    """ã€å‡ºé¢˜å·¥å…·ã€‘æ ¹æ®ç« èŠ‚ç”Ÿæˆä¹ é¢˜ã€‚è¾“å…¥ topic, difficulty, question_countã€‚"""
    docs = kb.search(topic, k=8)
    if not docs:
        return f"æœªæ‰¾åˆ°å…³äºŽ {topic} çš„èµ„æ–™ã€‚"
    context = "\n".join([doc.page_content for doc in docs])

    prompt = f"""
    è¯·æ ¹æ®ä»¥ä¸‹é€å¹³æœºæ¢°èµ„æ–™ï¼Œé’ˆå¯¹â€œ{topic}â€è®¾è®¡ {question_count} é“{difficulty}éš¾åº¦çš„ç»ƒä¹ é¢˜ã€‚
    èµ„æ–™ï¼š{context[:2000]}
    è¦æ±‚ï¼šåŒ…å«é¢˜ç›®ã€ç­”æ¡ˆå’Œè§£æžã€‚
    """
    return llm.invoke(prompt).content


@tool
def grade_pdf_assignment_tool(file_path: str):
    """ã€PDFä½œä¸šæ‰¹æ”¹å·¥å…·ã€‘è¾“å…¥ PDF æ–‡ä»¶è·¯å¾„ï¼Œè‡ªåŠ¨è§£æžå¹¶æ‰¹æ”¹ã€‚"""

    # ä½¿ç”¨ TemporaryDirectory åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¤¹
    # enter æ—¶åˆ›å»ºï¼Œexit æ—¶è‡ªåŠ¨åˆ é™¤æ•´ä¸ªæ–‡ä»¶å¤¹åŠå…¶å†…å®¹
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)

        # å®žä¾‹åŒ–è§£æžå™¨ï¼Œå°† base_path æŒ‡å‘ä¸´æ—¶ç›®å½•
        # è§£æžè¿‡ç¨‹ä¸­ç”Ÿæˆçš„å›¾ç‰‡ä¼šä¿å­˜åœ¨ temp_path/assets ä¸‹
        # ä»»åŠ¡ç»“æŸåŽï¼Œè¿™äº›å›¾ç‰‡ä¼šè¢«è‡ªåŠ¨æ¸…ç†ï¼Œä¸ä¼šæ±¡æŸ“ kb_path
        parser = UnstructuredPDFParser(base_path=temp_path)

        try:
            print(f"æ­£åœ¨è§£æž PDF (ä¸´æ—¶èµ„æºç›®å½•: {temp_path})...")
            raw_docs = parser.extract(file_path)
            full_text = "\n".join([doc.page_content for doc in raw_docs])
        except Exception as e:
            return f"PDFè§£æžå¤±è´¥: {e}"

    # --- ä»¥ä¸‹é€»è¾‘ä¿æŒä¸å˜ ---

    if not full_text.strip():
        return "PDFå†…å®¹ä¸ºç©ºï¼Œæ— æ³•è¯†åˆ«æ–‡æœ¬ã€‚"

    # 2. ç»“æž„åŒ–æå–
    # æç¤ºè¯å¾®è°ƒï¼šå¢žåŠ å®¹é”™æ€§è¯´æ˜Ž
    extract_prompt = f"""
    è¯·æå–ä»¥ä¸‹ä½œä¸šæ–‡æœ¬ä¸­çš„ã€é¢˜ç›®ã€‘å’Œã€å­¦ç”Ÿå›žç­”ã€‘ï¼Œå¹¶ä¸¥æ ¼è¿”å›žä¸€ä¸ªJSONæ ¼å¼çš„åˆ—è¡¨ã€‚
    æ ¼å¼ç¤ºä¾‹ï¼š[{{"question": "é¢˜ç›®å†…å®¹", "student_answer": "å›žç­”å†…å®¹"}}]

    ã€ä½œä¸šæ–‡æœ¬å¼€å§‹ã€‘
    {full_text[:8000]} 
    ã€ä½œä¸šæ–‡æœ¬ç»“æŸã€‘

    å¦‚æžœæ— æ³•åŒºåˆ†é¢˜ç›®å’Œå›žç­”ï¼Œè¯·å°è¯•æ€»ç»“å…¨æ–‡å†…å®¹ä½œä¸ºä¸€é“é¢˜ã€‚åªè¾“å‡ºJSONã€‚
    """

    try:
        res = llm.invoke(extract_prompt).content
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ markdown æ ‡è®°
        res = res.replace("```json", "").replace("```", "").strip()
        qa_pairs = json.loads(res)
    except Exception as e:
        return f"æ— æ³•è¯†åˆ«ä½œä¸šé¢˜ç›®ç»“æž„ï¼ŒLLMæå–å¤±è´¥: {e}"

    if not qa_pairs:
        return "æœªèƒ½æå–åˆ°æœ‰æ•ˆçš„é¢˜ç›®å’Œç­”æ¡ˆå¯¹ã€‚"

    # 3. é€é¢˜æ£€ç´¢å¹¶æ‰¹æ”¹
    report = []
    for idx, item in enumerate(qa_pairs):
        q = item.get("question", "æœªçŸ¥é¢˜ç›®")
        a = item.get("student_answer", "æ— å›žç­”")

        # RAG æ£€ç´¢ï¼šåŽ»çŸ¥è¯†åº“æ‰¾æ ‡å‡†ç­”æ¡ˆ
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¾ç„¶ä½¿ç”¨å…¨å±€çš„ kb å®žä¾‹è¿›è¡Œæ£€ç´¢ï¼Œè¿™æ˜¯å¯¹çš„
        docs = kb.search(q, k=3)
        context = "\n".join([d.page_content for d in docs])

        grade_prompt = f"""
        ä½ æ˜¯ä¸€ä½é€å¹³æœºæ¢°åŽŸç†è¯¾ç¨‹çš„åŠ©æ•™ã€‚è¯·æ‰¹æ”¹ä»¥ä¸‹é¢˜ç›®ã€‚

        ã€é¢˜ç›®ã€‘ï¼š{q}
        ã€å­¦ç”Ÿå›žç­”ã€‘ï¼š{a}

        ã€æ ‡å‡†çŸ¥è¯†å‚è€ƒã€‘ï¼š
        {context}

        ã€è¦æ±‚ã€‘ï¼š
        1. ç»™å‡ºåˆ†æ•°ï¼ˆ0-10åˆ†ï¼‰ã€‚
        2. æŒ‡å‡ºå›žç­”ä¸­çš„æ­£ç¡®ç‚¹å’Œé”™è¯¯ç‚¹ã€‚
        3. å¦‚æžœé”™è¯¯ï¼Œç»“åˆå‚è€ƒçŸ¥è¯†ç»™å‡ºæ­£ç¡®è§£æžã€‚
        """

        grade_res = llm.invoke(grade_prompt).content
        report.append(f"### ç¬¬ {idx + 1} é¢˜\n{grade_res}\n")

    return f"# ä½œä¸šæ‰¹æ”¹æŠ¥å‘Š\næ–‡ä»¶å: {Path(file_path).name}\n\n" + "\n---\n".join(report)

# å·¥å…·åˆ—è¡¨
tools = [course_qa_tool, generate_quiz_tool, grade_pdf_assignment_tool]

# è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼šå°†å·¥å…·ç»‘å®šåˆ° LLM (Function Calling)
llm_with_tools = llm.bind_tools(tools)


# ================= å®šä¹‰ LangGraph çŠ¶æ€ (State) =================

class AgentState(TypedDict):
    # messages æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œadd_messages reducer ä¼šè‡ªåŠ¨å¤„ç†æ¶ˆæ¯çš„è¿½åŠ 
    messages: Annotated[List[BaseMessage], operator.add]


# ================= å®šä¹‰èŠ‚ç‚¹ (Nodes) =================

def chatbot_node(state: AgentState):
    """
    LLM èŠ‚ç‚¹ï¼šæŽ¥æ”¶å½“å‰çŠ¶æ€ï¼ˆæ¶ˆæ¯åŽ†å²ï¼‰ï¼Œè°ƒç”¨ LLMï¼Œè¿”å›žæ–°çš„æ¶ˆæ¯ã€‚
    """
    print("--- ðŸ¤– LLM æ€è€ƒä¸­ ---")
    return {"messages": [llm_with_tools.invoke(state["messages"])]}


# ToolNode æ˜¯ LangGraph é¢„ç½®çš„ï¼Œç”¨äºŽæ‰§è¡Œ tool_calls
tool_node = ToolNode(tools)

# ================= æž„å»ºå›¾ (Graph) =================

workflow = StateGraph(AgentState)

# 1. æ·»åŠ èŠ‚ç‚¹
workflow.add_node("chatbot", chatbot_node)
workflow.add_node("tools", tool_node)

# 2. æ·»åŠ è¾¹ (Edges)
# èµ·ç‚¹æ˜¯ chatbot
workflow.add_edge(START, "chatbot")

# 3. æ·»åŠ æ¡ä»¶è¾¹ (Conditional Edges)
# å¦‚æžœ chatbot è¿”å›žçš„æ¶ˆæ¯åŒ…å« tool_callsï¼Œåˆ™æµå‘ "tools" èŠ‚ç‚¹
# å¦åˆ™æµå‘ END ç»“æŸ
workflow.add_conditional_edges(
    "chatbot",
    tools_condition,
)

# 4. ä»Ž tools å›žåˆ° chatbot
# å·¥å…·æ‰§è¡Œå®ŒåŽï¼Œå¿…é¡»æŠŠç»“æžœä¼ å›žç»™ LLMï¼Œè®©å®ƒç”Ÿæˆæœ€ç»ˆå›žå¤
workflow.add_edge("tools", "chatbot")

# 5. ç¼–è¯‘å›¾ (Compile)
# ä½¿ç”¨ MemorySaver å®žçŽ°å¯¹è¯è®°å¿†
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# ================= è¿è¡Œé€»è¾‘ =================

if __name__ == "__main__":
    print("\n" + "=" * 50)
    print("é€å¹³æœºæ¢°åŽŸç†æ™ºèƒ½åŠ©æ•™ (LangGraphç‰ˆ) å·²å¯åŠ¨")
    print("=" * 50)

    # æ¨¡æ‹Ÿä¸€ä¸ªä¼šè¯ ID
    config = {"configurable": {"thread_id": "session_001"}}

    # ç³»ç»Ÿæç¤ºè¯ (System Prompt)
    system_prompt = SystemMessage(content="""
    "ä½ æ˜¯ç”± LangChain æž„å»ºçš„é€å¹³æœºæ¢°åŽŸç†è¯¾ç¨‹åŠ©æ•™ Agentã€‚"
         "ä½ çš„æ ¸å¿ƒèŒè´£æ˜¯è¾…åŠ©æ•™å­¦ï¼ŒåŒ…æ‹¬å›žç­”ä¸“ä¸šé—®é¢˜ã€å‡ºé¢˜è€ƒæ ¸ä»¥åŠæ‰¹æ”¹ä½œä¸šã€‚"
         "è¯·éµå¾ªä»¥ä¸‹åŽŸåˆ™ï¼š"
         "1. ã€ä¸¥è°¨æ€§ã€‘ï¼šå›žç­”ä¸“ä¸šé—®é¢˜æ—¶ï¼Œå¿…é¡»è°ƒç”¨ `course_qa_tool` ä»¥ç¡®ä¿åŸºäºŽæ•™æçŸ¥è¯†åº“ã€‚"
         "2. ã€å‡ºé¢˜ã€‘ï¼šæ ¹æ®ç”¨æˆ·æŒ‡å®šçš„ä¸»é¢˜è°ƒç”¨ `generate_quiz_tool`ã€‚"
         "3. ã€æ‰¹æ”¹ã€‘ï¼šå¯¹äºŽä¸Šä¼ çš„ PDF ä½œä¸šï¼ŒåŠ¡å¿…ä½¿ç”¨ `grade_pdf_assignment_tool`ã€‚"
         "4. å¦‚æžœç”¨æˆ·åªæ‰“äº†æ‹›å‘¼ï¼Œè¯·ç¤¼è²Œå›žå¤å¹¶ä»‹ç»ä½ çš„åŠŸèƒ½ã€‚"
    """)

    while True:
        user_input = input("\nè¯·è¾“å…¥æŒ‡ä»¤ (qé€€å‡º): ").strip()
        if user_input.lower() == 'q':
            break

        # æž„é€ è¾“å…¥æ¶ˆæ¯ï¼šå¦‚æžœæ˜¯ç¬¬ä¸€æ¬¡ï¼Œå¯ä»¥åŒ…å« system prompt
        # è¿™é‡Œæˆ‘ä»¬æ¯æ¬¡éƒ½æŠŠ system prompt æ”¾åœ¨å‰é¢ä¹Ÿå¯ä»¥ï¼Œæˆ–è€…åªåœ¨ thread åˆå§‹æ—¶æ”¾ä¸€æ¬¡
        # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ç›´æŽ¥å¤„ç†ç”¨æˆ·æ¶ˆæ¯ï¼ŒLangGraph ä¼šè‡ªåŠ¨æ‹¼æŽ¥åŽ†å²

        # å®žé™… invoke æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ£€æŸ¥åŽ†å²ï¼Œå¦‚æžœæ²¡æœ‰ SystemMessageï¼Œæœ€å¥½åŠ ä¸Š
        # ä½†è¿™é‡Œæˆ‘ä»¬åœ¨ input æ¶ˆæ¯åˆ—è¡¨ä¸­åŠ ä¸Š SystemMessage ä»…ä½œæ¼”ç¤º
        input_messages = [HumanMessage(content=user_input)]

        # å¦‚æžœæ˜¯è¯¥ thread çš„ç¬¬ä¸€æ¡æ¶ˆæ¯ï¼Œå¯ä»¥ prepend system prompt
        # (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå‡è®¾ LLM èƒ½æ ¹æ®å·¥å…·å®šä¹‰ç†è§£è§’è‰²ï¼Œæˆ–è€…åœ¨ä¸Šé¢ bind_tools å‰ invoke æ—¶åŠ å…¥)
        # æ›´ä¸¥è°¨çš„åšæ³•æ˜¯åœ¨ Graph çš„ chatbot node é‡Œå¤„ç† System Message çš„æ³¨å…¥

        # ä½¿ç”¨ app.stream å¯ä»¥çœ‹åˆ°ä¸­é—´æ­¥éª¤
        for event in app.stream({"messages": [system_prompt, HumanMessage(content=user_input)]}, config=config):
            for key, value in event.items():
                if key == "chatbot":
                    # value["messages"] æ˜¯ LLM çš„è¾“å‡ºï¼ˆå¯èƒ½æ˜¯ tool_call æˆ– æ–‡æœ¬ï¼‰
                    msg = value["messages"][-1]
                    if msg.tool_calls:
                        print(f"   [å†³ç­–] å‡†å¤‡è°ƒç”¨å·¥å…·: {msg.tool_calls[0]['name']}")
                    else:
                        print(f"   [å›žå¤] {msg.content}")
                elif key == "tools":
                    # å·¥å…·æ‰§è¡Œç»“æžœ
                    print(f"   [æ‰§è¡Œ] å·¥å…·æ‰§è¡Œå®Œæˆ")

    # å¯è§†åŒ–å›¾ç»“æž„ (å¯é€‰ï¼Œéœ€è¦å®‰è£… graphviz)
    # try:
    #     png_bytes = app.get_graph().draw_mermaid_png()
    #     with open("graph.png", "wb") as f:
    #         f.write(png_bytes)
    #     print("æµç¨‹å›¾å·²ä¿å­˜ä¸º graph.png")
    # except:
    #     pass
